{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from commons import load_glove_embedding, pad_input, load_imdb, get_max_length, Rating, WordIndex\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "keras = tf.keras\n",
    "\n",
    "print(\"Tensorflow Version: \" + tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# imdb data\n",
    "imdb = load_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb\n",
    "word_index = WordIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# GLOVE Word Embedding\n",
    "GLOVE_DIR = \"D:/google drive/haw/master/mastertheisis/hauptprojekt\"\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_index = load_glove_embedding(GLOVE_DIR, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17361/88587 unknown words\n"
     ]
    }
   ],
   "source": [
    "(embedding_matrix, unknown_words) = word_index.match_glove(embedding_index=embedding_index, embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = get_max_length(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 11, 19, ...,  0,  0,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad input vectors\n",
    "x_train_padded = pad_input(x_train, max_length)\n",
    "x_test_padded = pad_input(x_test, max_length)\n",
    "x_train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tom\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Tom\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2697, 50)          4429400   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2697, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 2683, 64)          48064     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2683, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2679, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 76, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 76, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                311360    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,809,433\n",
      "Trainable params: 380,033\n",
      "Non-trainable params: 4,429,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(len(word_index.index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv1D(64, 15, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv1D(64, 5, activation=\"relu\"),\n",
    "    keras.layers.MaxPooling1D(35),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Tom\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/8\n",
      "23750/23750 [==============================] - 22s 936us/sample - loss: 0.5859 - acc: 0.6847 - val_loss: 0.5192 - val_acc: 0.7368\n",
      "Epoch 2/8\n",
      "23750/23750 [==============================] - 20s 859us/sample - loss: 0.4949 - acc: 0.7686 - val_loss: 0.4747 - val_acc: 0.7816\n",
      "Epoch 3/8\n",
      "23750/23750 [==============================] - 20s 863us/sample - loss: 0.4550 - acc: 0.7927 - val_loss: 0.4379 - val_acc: 0.8088\n",
      "Epoch 4/8\n",
      "23750/23750 [==============================] - 21s 864us/sample - loss: 0.4318 - acc: 0.8035 - val_loss: 0.4032 - val_acc: 0.8336\n",
      "Epoch 5/8\n",
      "23750/23750 [==============================] - 21s 863us/sample - loss: 0.4158 - acc: 0.8112 - val_loss: 0.3804 - val_acc: 0.8512\n",
      "Epoch 6/8\n",
      "23750/23750 [==============================] - 21s 865us/sample - loss: 0.3972 - acc: 0.8251 - val_loss: 0.3946 - val_acc: 0.8400\n",
      "Epoch 7/8\n",
      "23750/23750 [==============================] - 21s 884us/sample - loss: 0.3855 - acc: 0.8266 - val_loss: 0.3674 - val_acc: 0.8456\n",
      "Epoch 8/8\n",
      "23750/23750 [==============================] - 21s 892us/sample - loss: 0.3861 - acc: 0.8260 - val_loss: 0.3749 - val_acc: 0.8472\n",
      "25000/25000 [==============================] - 7s 289us/sample - loss: 0.3189 - acc: 0.8832\n"
     ]
    }
   ],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs', \n",
    "    histogram_freq=0,\n",
    "    write_graph=True, \n",
    "    write_images=True) \n",
    "\n",
    "model.fit(x_train_padded, y_train, validation_split=0.05, epochs=8, callbacks=[tb_callback])\n",
    "loss, accuracy = model.evaluate(x_train_padded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(x, y):\n",
    "    test_result = np.round(model.predict(x))\n",
    "    test_errors = np.squeeze(test_result) != y\n",
    "    correct_percentage = np.sum(test_errors) / len(y)\n",
    "    print(\"%i / %i (%.2f%%) are correct\" % (len(y) - np.sum(test_errors), len(y), 100 * (1 - correct_percentage)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21017 / 25000 (84.07%) are correct\n",
      "22081 / 25000 (88.32%) are correct\n"
     ]
    }
   ],
   "source": [
    "test_model(x_test_padded, y_test)\n",
    "test_model(x_train_padded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '<UNK>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f03c83a6cb8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mrating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcommons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mrating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Tom\\Documents\\gitworkspace\\master\\ml-probability\\tfp-word-embeddings\\commons.py\u001b[0m in \u001b[0;36mof\u001b[1;34m(self, sentences, max_length)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         encoded_custom_sentences = pad_input(\n\u001b[1;32m--> 172\u001b[1;33m             list(map(self.word_index.sentence2vec, sentences)), max_length)\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[0mcustom_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_custom_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tom\\Documents\\gitworkspace\\master\\ml-probability\\tfp-word-embeddings\\commons.py\u001b[0m in \u001b[0;36mpad_input\u001b[1;34m(vec, max_length)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[0mmax_length\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mmax\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \"\"\"\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m# check `trunc` has expected shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mtrunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '<UNK>'"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"this was a very bad movie\",\n",
    "    \"this was a very good movie\",\n",
    "    \"I did not like this movie at all\",\n",
    "    \"I hope there will be a sequal\",\n",
    "    \"not bad\",\n",
    "    \"bad\",\n",
    "    \"not good\",\n",
    "    \"one of the best movies of the year\",\n",
    "    \"the first part was bad but the second part got better\",\n",
    "    \"the first part was not bad but after that it just got worse\",\n",
    "    \"this film was just brilliant casting location scenery story direction everyone's really suited part they played you could just imagine being there robert redford's is an amazing actor now same being director norman's father came from same scottish island as myself so i loved fact there was a real connection with this film witty remarks throughout film were great it was just brilliant so much that i bought\"\n",
    "]\n",
    "\n",
    "rating = commons.Rating(word_index, model)\n",
    "rating.print(rating.of(sentences, max_length))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
