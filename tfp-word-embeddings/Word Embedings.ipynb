{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "keras = tf.keras\n",
    "\n",
    "print(\"Tensorflow Version: \" + tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Dataset \n",
    "https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# imdb data\n",
    "INDEX_FROM = 0\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(index_from=INDEX_FROM)\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "\n",
    "word_index_inv = {v: k for k, v in word_index.items()}\n",
    "\n",
    "def encoded_vector2sentence(vec, word_index_inv = word_index_inv):\n",
    "    sentence = map(lambda i: word_index_inv[i], vec)\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "def pad_input(vec, maxlen):\n",
    "    return keras.preprocessing.sequence.pad_sequences(vec, maxlen=maxlen, padding='post')\n",
    "\n",
    "def sentence2encoded_vec(sentence):\n",
    "    sentence_arr = sentence.split(\" \")\n",
    "    return list(map(lambda word: word_index[word.lower()], sentence_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Pretrained Word Embedding\n",
    "GLOVE_DIR = \"D:/google drive/haw/master/mastertheisis/hauptprojekt\"\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16278/88587 unknown words\n"
     ]
    }
   ],
   "source": [
    "# match the word index with the word_embedding index\n",
    "import re\n",
    "\n",
    "regex = re.compile('[^a-zA-Z0-9]')\n",
    "def normalize_word(word):\n",
    "    #return regex.sub('', word)\n",
    "    return word.replace(\"'d\", \"\").replace(\"'s\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "n_unknown_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(normalize_word(word))\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        n_unknown_words += 1\n",
    "\n",
    "print(\"{}/{} unknown words\".format(n_unknown_words, len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 19, 13, 40, 527, 970, 1619, 1382, 62, 455, 4465, 63, 3938, 1, 170, 33, 253, 2, 22, 97, 40, 835, 109, 47, 667, 22662, 6, 32, 477, 281, 2, 147, 1, 169, 109, 164, 21628, 333, 382, 36, 1, 169, 4533, 1108, 14, 543, 35, 10, 444, 1, 189, 47, 13, 3, 144, 2022, 16, 11, 19, 1, 1917, 4610, 466, 1, 19, 68, 84, 9, 13, 40, 527, 35, 73, 12, 10, 1244, 1, 19, 14, 512, 14, 9, 13, 623, 15, 19190, 2, 59, 383, 9, 5, 313, 5, 103, 2, 1, 2220, 5241, 13, 477, 63, 3782, 30, 1, 127, 9, 13, 35, 616, 2, 22, 121, 48, 33, 132, 45, 22, 1412, 30, 3, 19, 9, 212, 25, 74, 49, 2, 11, 404, 13, 79, 10308, 5, 1, 104, 114, 5949, 12, 253, 1, 31047, 4, 3763, 2, 720, 33, 68, 40, 527, 473, 23, 397, 314, 43, 4, 1, 12115, 1026, 10, 101, 85, 1, 378, 12, 294, 95, 29, 2068, 53, 23, 138, 3, 191, 7483, 15, 1, 223, 19, 18, 131, 473, 23, 477, 2, 141, 27, 5532, 15, 48, 33, 25, 221, 89, 22, 101, 1, 223, 62, 13, 35, 1331, 85, 9, 13, 280, 2, 13, 4469, 110, 100, 29, 12, 13, 5342, 16, 175, 29] 1\n"
     ]
    }
   ],
   "source": [
    "# raw data representation\n",
    "print(x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited <START> part they played <UNK> you could just imagine being there robert redford's is an amazing actor <UNK> now <START> same being director norman's father came from <START> same scottish island as myself so i loved <START> fact there was a real connection with this film <START> witty remarks throughout <START> film were great it was just brilliant so much that i bought <START> film as soon as it was released for retail <UNK> would recommend it to everyone to watch <UNK> <START> fly fishing was amazing really cried at <START> end it was so sad <UNK> you know what they say if you cry at a film it must have been good <UNK> this definitely was also congratulations to <START> two little boy's that played <START> part's of norman <UNK> paul they were just brilliant children are often left out of <START> praising list i think because <START> stars that play them all grown up are such a big profile for <START> whole film but these children are amazing <UNK> should be praised for what they have done don't you think <START> whole story was so lovely because it was true <UNK> was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# parsed sentence\n",
    "print(encoded_vector2sentence(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Input Length\n",
    "max_length = max(map(lambda vec: len(vec), x_train + x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 11, 19, ...,  0,  0,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad input vectors\n",
    "x_train_padded = pad_input(x_train, max_length)\n",
    "x_test_padded = pad_input(x_test, max_length)\n",
    "x_train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 2697, 50)          4429400   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 2697, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 2683, 64)          48064     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 2683, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 2679, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 76, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 76, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                311360    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,809,433\n",
      "Trainable params: 380,033\n",
      "Non-trainable params: 4,429,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv1D(64, 15, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv1D(64, 5, activation=\"relu\"),\n",
    "    keras.layers.MaxPooling1D(35),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/8\n",
      "23750/23750 [==============================] - 22s 910us/sample - loss: 0.6118 - acc: 0.6446 - val_loss: 0.5605 - val_acc: 0.7368\n",
      "Epoch 2/8\n",
      "23750/23750 [==============================] - 21s 886us/sample - loss: 0.5058 - acc: 0.7567 - val_loss: 0.4876 - val_acc: 0.7664\n",
      "Epoch 3/8\n",
      "23750/23750 [==============================] - 21s 884us/sample - loss: 0.4722 - acc: 0.7777 - val_loss: 0.4653 - val_acc: 0.7792\n",
      "Epoch 4/8\n",
      "23750/23750 [==============================] - 21s 887us/sample - loss: 0.4479 - acc: 0.7950 - val_loss: 0.4315 - val_acc: 0.8040\n",
      "Epoch 5/8\n",
      "23750/23750 [==============================] - 21s 887us/sample - loss: 0.4280 - acc: 0.8048 - val_loss: 0.3965 - val_acc: 0.8480\n",
      "Epoch 6/8\n",
      "23750/23750 [==============================] - 21s 892us/sample - loss: 0.4116 - acc: 0.8128 - val_loss: 0.4121 - val_acc: 0.8224\n",
      "Epoch 7/8\n",
      "23750/23750 [==============================] - 21s 873us/sample - loss: 0.3975 - acc: 0.8211 - val_loss: 0.4453 - val_acc: 0.7904\n",
      "Epoch 8/8\n",
      "23750/23750 [==============================] - 21s 881us/sample - loss: 0.3888 - acc: 0.8285 - val_loss: 0.3977 - val_acc: 0.8344\n",
      "25000/25000 [==============================] - 7s 299us/sample - loss: 0.3458 - acc: 0.8544\n"
     ]
    }
   ],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs', \n",
    "    histogram_freq=0,\n",
    "    write_graph=True, \n",
    "    write_images=True) \n",
    "\n",
    "model.fit(x_train_padded, y_train, validation_split=0.05, epochs=8, callbacks=[tb_callback])\n",
    "loss, accuracy = model.evaluate(x_train_padded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(x, y):\n",
    "    test_result = np.round(model.predict(x))\n",
    "    test_errors = np.squeeze(test_result) != y\n",
    "    correct_percentage = np.sum(test_errors) / len(y)\n",
    "    print(\"%i / %i (%.2f%%) are correct\" % (len(y) - np.sum(test_errors), len(y), 100 * (1 - correct_percentage)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20551 / 25000 (82.20%) are correct\n",
      "21360 / 25000 (85.44%) are correct\n"
     ]
    }
   ],
   "source": [
    "test_model(x_test_padded, y_test)\n",
    "test_model(x_train_padded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⭐⭐ (17.76%)\n",
      "this was a very bad movie\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐⭐⭐ (92.86%)\n",
      "this was a very good movie\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐ (71.37%)\n",
      "I did not like this movie at all\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐ (65.06%)\n",
      "I hope there will be a sequal\n",
      "\n",
      "⭐⭐⭐⭐⭐ (52.46%)\n",
      "not bad\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐ (62.59%)\n",
      "bad\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐ (67.94%)\n",
      "not good\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐⭐⭐ (89.08%)\n",
      "one of the best movies of the year\n",
      "\n",
      "⭐⭐⭐⭐ (41.84%)\n",
      "the first part was bad but the second part got better\n",
      "\n",
      "⭐ (14.06%)\n",
      "the first part was not bad but after that it just got worse\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐ (96.61%)\n",
      "this film was just brilliant casting location scenery story direction everyone's really suited part they played you could just imagine being there robert redford's is an amazing actor now same being director norman's father came from same scottish island as myself so i loved fact there was a real connection with this film witty remarks throughout film were great it was just brilliant so much that i bought\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_sentences = [\n",
    "    \"this was a very bad movie\",\n",
    "    \"this was a very good movie\",\n",
    "    \"I did not like this movie at all\",\n",
    "    \"I hope there will be a sequal\",\n",
    "    \"not bad\",\n",
    "    \"bad\",\n",
    "    \"not good\",\n",
    "    \"one of the best movies of the year\",\n",
    "    \"the first part was bad but the second part got better\",\n",
    "    \"the first part was not bad but after that it just got worse\",\n",
    "    \"this film was just brilliant casting location scenery story direction everyone's really suited part they played you could just imagine being there robert redford's is an amazing actor now same being director norman's father came from same scottish island as myself so i loved fact there was a real connection with this film witty remarks throughout film were great it was just brilliant so much that i bought\"\n",
    "]\n",
    "\n",
    "\n",
    "def create_rating(sentences):\n",
    "    encoded_custom_sentences = pad_input(list(map(sentence2encoded_vec, custom_sentences)), max_length)\n",
    "    custom_result = model.predict(encoded_custom_sentences)\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(custom_sentences)):\n",
    "        sentence = custom_sentences[i]\n",
    "        prediction = custom_result[i][0]\n",
    "        rating = int(round(prediction * 10))\n",
    "        result += [(sentence, prediction, rating)]\n",
    "    return result\n",
    "                  \n",
    "\n",
    "for (sentence, prediction, rating) in create_rating(custom_sentences):\n",
    "    print(\"%s (%.2f%%)\\n%s\\n\" % (rating * \"⭐\", prediction * 100, sentence)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
