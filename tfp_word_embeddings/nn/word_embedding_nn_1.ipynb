{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "keras = tf.keras\n",
    "\n",
    "print(\"Tensorflow Version: %s\" % tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'commons' from 'D:\\\\Tom\\\\Documents\\\\gitworkspace\\\\master\\\\ml-probability\\\\tfp_word_embeddings\\\\commons.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# commons package\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import commons as cm\n",
    "importlib.reload(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# imdb data\n",
    "imdb = cm.load_imdb()\n",
    "(x_train, y_train), (x_test, y_test) = imdb\n",
    "word_index = cm.WordIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# GLOVE Word Embedding\n",
    "GLOVE_DIR = \"D:/google drive/haw/master/mastertheisis/hauptprojekt\"\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_index = cm.load_glove_embedding(GLOVE_DIR, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17361/88587 unknown words\n"
     ]
    }
   ],
   "source": [
    "(embedding_matrix, unknown_words) = word_index.match_glove(embedding_index=embedding_index, embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = cm.get_max_length(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 11, 19, ...,  0,  0,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad input vectors\n",
    "x_train_padded = cm.pad_input(x_train, max_length)\n",
    "x_test_padded = cm.pad_input(x_test, max_length)\n",
    "x_train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2697, 50)          4429400   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2697, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2683, 64)          48064     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2683, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2679, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 267, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 267, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 17088)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 17088)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                1093696   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,591,769\n",
      "Trainable params: 1,162,369\n",
      "Non-trainable params: 4,429,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(len(word_index.index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv1D(64, 15, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv1D(64, 5, activation=\"relu\"),\n",
    "    keras.layers.MaxPooling1D(10),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Tom\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/8\n",
      "23750/23750 [==============================] - 23s 980us/sample - loss: 0.5901 - acc: 0.6794 - val_loss: 0.5080 - val_acc: 0.7720\n",
      "Epoch 2/8\n",
      "23750/23750 [==============================] - 21s 890us/sample - loss: 0.4955 - acc: 0.7680 - val_loss: 0.4603 - val_acc: 0.8112\n",
      "Epoch 3/8\n",
      "23750/23750 [==============================] - 21s 893us/sample - loss: 0.4586 - acc: 0.7897 - val_loss: 0.4261 - val_acc: 0.8152\n",
      "Epoch 4/8\n",
      "23750/23750 [==============================] - 22s 927us/sample - loss: 0.4277 - acc: 0.8052 - val_loss: 0.3990 - val_acc: 0.8400\n",
      "Epoch 5/8\n",
      "23750/23750 [==============================] - 21s 905us/sample - loss: 0.4079 - acc: 0.8155 - val_loss: 0.3845 - val_acc: 0.8472\n",
      "Epoch 6/8\n",
      "23750/23750 [==============================] - 21s 894us/sample - loss: 0.3990 - acc: 0.8243 - val_loss: 0.3927 - val_acc: 0.8456\n",
      "Epoch 7/8\n",
      "23750/23750 [==============================] - 22s 913us/sample - loss: 0.3876 - acc: 0.8253 - val_loss: 0.3777 - val_acc: 0.8560\n",
      "Epoch 8/8\n",
      "23750/23750 [==============================] - 22s 914us/sample - loss: 0.3717 - acc: 0.8354 - val_loss: 0.3756 - val_acc: 0.8528\n",
      "25000/25000 [==============================] - 7s 278us/sample - loss: 0.3110 - acc: 0.8882\n"
     ]
    }
   ],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs', \n",
    "    histogram_freq=0,\n",
    "    write_graph=True, \n",
    "    write_images=True) \n",
    "\n",
    "model.fit(x_train_padded, y_train, validation_split=0.05, epochs=8, callbacks=[tb_callback])\n",
    "loss, accuracy = model.evaluate(x_train_padded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(x, y):\n",
    "    test_result = np.round(model.predict(x))\n",
    "    test_errors = np.squeeze(test_result) != y\n",
    "    correct_percentage = np.sum(test_errors) / len(y)\n",
    "    print(\"%i / %i (%.2f%%) are correct\" % (len(y) - np.sum(test_errors), len(y), 100 * (1 - correct_percentage)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21097 / 25000 (84.39%) are correct\n",
      "22205 / 25000 (88.82%) are correct\n"
     ]
    }
   ],
   "source": [
    "test_model(x_test_padded, y_test)\n",
    "test_model(x_train_padded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⭐ (6.62%)\n",
      "this was a very bad movie\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐⭐⭐ (94.83%)\n",
      "this was a very good movie\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐ (67.09%)\n",
      "I did not like this movie at all\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐ (60.73%)\n",
      "I hope there will be a sequal\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐ (57.35%)\n",
      "not bad\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐ (58.33%)\n",
      "bad\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐ (70.76%)\n",
      "not good\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐⭐ (79.93%)\n",
      "one of the best movies of the year\n",
      "\n",
      "⭐⭐⭐ (29.11%)\n",
      "the first part was bad but the second part got better\n",
      "\n",
      "⭐⭐ (21.82%)\n",
      "the first part was not bad but after that it just got worse\n",
      "\n",
      "⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐ (96.65%)\n",
      "this film was just brilliant casting location scenery story direction everyone's really suited part they played you could just imagine being there robert redford's is an amazing actor now same being director norman's father came from same scottish island as myself so i loved fact there was a real connection with this film witty remarks throughout film were great it was just brilliant so much that i bought\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"this was a very bad movie\",\n",
    "    \"this was a very good movie\",\n",
    "    \"I did not like this movie at all\",\n",
    "    \"I hope there will be a sequal\",\n",
    "    \"not bad\",\n",
    "    \"bad\",\n",
    "    \"not good\",\n",
    "    \"one of the best movies of the year\",\n",
    "    \"the first part was bad but the second part got better\",\n",
    "    \"the first part was not bad but after that it just got worse\",\n",
    "    \"this film was just brilliant casting location scenery story direction everyone's really suited part they played you could just imagine being there robert redford's is an amazing actor now same being director norman's father came from same scottish island as myself so i loved fact there was a real connection with this film witty remarks throughout film were great it was just brilliant so much that i bought\"\n",
    "]\n",
    "\n",
    "rating = cm.Rating(word_index, model)\n",
    "rating.print(rating.of(sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
